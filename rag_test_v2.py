# -*- coding: utf-8 -*-
"""rag_test_v2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1BMCk-8qa9V04Ar6dzXPBJvVmH5Q8cVyR

## **Installing pre-reqs**
"""

!pip -q install faiss-cpu sentence-transformers langchain langchain-community langchain-text-splitters pypdf python-docx llama-cpp-python pandas tqdm requests

"""## **Setting up google drive and folders**"""

from google.colab import drive
drive.mount('/content/drive')  # uncomment if you want persistence

import os, pathlib
BASE_DIR = "/content/drive/MyDrive/Colab"
DATA_DIR = f"{BASE_DIR}/dataset_raw"   # raw zips/files
DOCS_DIR = f"{BASE_DIR}/docs"          # normalized text
OUT_DIR  = f"{BASE_DIR}/rag_outputs"   # indices, exports, model
for d in (DATA_DIR, DOCS_DIR, OUT_DIR): pathlib.Path(d).mkdir(parents=True, exist_ok=True)

"""## **Getting data from zenodo**"""

import requests, os, json, re, shutil, zipfile
from pathlib import Path

ZENODO_REC_ID = "13880060"  # Requirements data sets (user stories)
api_url = f"https://zenodo.org/api/records/{ZENODO_REC_ID}"
rec = requests.get(api_url, timeout=60).json()

files = rec.get("files", []) or rec.get("metadata", {}).get("related_identifiers", [])  # fallback
downloaded = []

if "files" in rec:
    for f in rec["files"]:
        url = f["links"]["self"]
        name = f["key"]
        dest = Path(DATA_DIR)/name
        if not dest.exists():
            with requests.get(url, stream=True, timeout=120) as r:
                r.raise_for_status()
                with open(dest, "wb") as fh:
                    shutil.copyfileobj(r.raw, fh)
        downloaded.append(str(dest))
else:
    print("Could not find file list in record JSON; open the Zenodo page and download manually.")

print(f"Downloaded {len(downloaded)} files")
downloaded[:8]

for p in list(Path(DATA_DIR).glob("*.zip")):
    with zipfile.ZipFile(p, 'r') as zf:
        zf.extractall(DATA_DIR)
    print("Extracted:", p.name)

import gzip, shutil
from pathlib import Path

RAW_DIR      = "/content/drive/MyDrive/Colab/dataset_raw"   # your raw data folder
EXTRACT_DIR  = "/content/drive/MyDrive/Colab/extracted_raw" # new folder for decompressed text

Path(EXTRACT_DIR).mkdir(parents=True, exist_ok=True)

def is_gzip_sig(path):
    with open(path, "rb") as f:
        return f.read(3) == b"\x1f\x8b\x08"  # gzip magic number

decompressed_files = []

for p in Path(RAW_DIR).rglob("*"):
    if not p.is_file():
        continue
    # check extension OR signature
    if p.suffix.lower() == ".gz" or is_gzip_sig(str(p)):
        out_name = p.stem  # remove .gz extension
        out_path = Path(EXTRACT_DIR) / (out_name + ".txt")

        try:
            with gzip.open(p, "rb") as f_in, open(out_path, "wb") as f_out:
                shutil.copyfileobj(f_in, f_out)
            decompressed_files.append(out_path)
            print(f"✅ Decompressed: {p.name} → {out_path.name}")
        except Exception as e:
            print(f"⚠️ Failed on {p.name}: {e}")

print(f"\nTotal decompressed: {len(decompressed_files)}")

"""## **Normalize everything to plain text**"""

import re, glob
from pypdf import PdfReader
from docx import Document

def file_to_text(path):
    p = path.lower()
    if p.endswith(".pdf"):
        try:
            pages = []
            reader = PdfReader(path)
            for i, pg in enumerate(reader.pages, start=1):
                t = pg.extract_text() or ""
                pages.append(f"[PAGE {i}] {t}")
            return "\n".join(pages)
        except: return ""
    if p.endswith(".docx"):
        try:
            doc = Document(path)
            return "\n".join(p.text for p in doc.paragraphs)
        except: return ""
    if any(p.endswith(ext) for ext in (".txt",".md",".rst",".csv")):
        try:
            with open(path, "r", errors="ignore") as f: return f.read()
        except: return ""
    return ""  # unsupported

def clean_text(s):
    s = s.replace("\x00"," ")
    s = re.sub(r"[ \t]+"," ", s)
    s = re.sub(r"\n{3,}","\n\n", s)
    return s.strip()

import os, pathlib
count=0
for root, _, files in os.walk(EXTRACT_DIR):
    for fn in files:
        src = os.path.join(root, fn)
        txt = clean_text(file_to_text(src))
        if txt and len(txt) > 50:
            out = f"{DOCS_DIR}/{pathlib.Path(fn).stem}.txt"
            with open(out, "w") as f: f.write(txt)
            count += 1
print("Normalized text files:", count)

"""## **Chunking, Embedding, FAISS**"""

from langchain_text_splitters import RecursiveCharacterTextSplitter
from sentence_transformers import SentenceTransformer
import numpy as np, faiss, pickle, glob, os

# Load normalized docs
docs = []
for fp in sorted(glob.glob(f"{DOCS_DIR}/*.txt")):
    with open(fp, "r", errors="ignore") as f:
        docs.append({"source": os.path.basename(fp), "text": f.read()})
print("Docs:", len(docs))

# Chunk
splitter = RecursiveCharacterTextSplitter(
    chunk_size=1200, chunk_overlap=150,
    separators=["\n## ","\n# ","\n\n","\n"," ",""]
)
chunks = []
for d in docs:
    for ch in splitter.split_text(d["text"]):
        chunks.append({"source": d["source"], "text": ch})
print("Chunks:", len(chunks))

# Embeddings + FAISS (cosine via normalized IP)
EMB_NAME = "sentence-transformers/all-MiniLM-L6-v2"
embed_model = SentenceTransformer(EMB_NAME)

B=128; vecs=[]
texts = [c["text"] for c in chunks]
for i in range(0, len(texts), B):
    vecs.append(embed_model.encode(texts[i:i+B], convert_to_numpy=True, normalize_embeddings=True))
vecs = np.vstack(vecs).astype("float32")

index = faiss.IndexFlatIP(vecs.shape[1])
index.add(vecs)

# Persist
with open(f"{OUT_DIR}/chunks.pkl","wb") as f: pickle.dump(chunks,f)
faiss.write_index(index, f"{OUT_DIR}/faiss.index")
with open(f"{OUT_DIR}/emb_model.txt","w") as f: f.write(EMB_NAME)
print("Index built.")

"""**Retriever helper**"""

def retrieve(query, k=8):
    q = embed_model.encode([query], convert_to_numpy=True, normalize_embeddings=True).astype("float32")
    D, I = index.search(q, k)
    hits=[]
    for rank,(idx,score) in enumerate(zip(I[0],D[0]), start=1):
        hits.append({"rank":rank,"score":float(score),"source":chunks[idx]["source"],"text":chunks[idx]["text"]})
    return hits

"""## **Local LLM via llama_cpp**

"""

import os, subprocess, pathlib

MODEL_URL = "https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.2-GGUF/resolve/main/mistral-7b-instruct-v0.2.Q4_K_M.gguf"
MODEL_PATH = f"{OUT_DIR}/mistral-7b-instruct.Q4_K_M.gguf"
if not os.path.exists(MODEL_PATH):
    !wget -q -O "$MODEL_PATH" "$MODEL_URL"

from llama_cpp import Llama
llm = Llama(model_path=MODEL_PATH, n_ctx=8192, n_threads=8, n_gpu_layers=2, temperature=0.2)

def chat_local(messages, max_tokens=1024, temperature=0.2):
    out = llm.create_chat_completion(messages=messages, temperature=temperature, max_tokens=max_tokens)
    return out["choices"][0]["message"]["content"]

"""## **Prompt and RAG generation**"""

TESTCASE_SYSTEM = """You are a senior QA engineer. Generate rigorous, unambiguous test cases
based ONLY on the provided context snippets. If context is missing details, list assumptions and gaps."""

TESTCASE_USER_TEMPLATE = """Context (RAG snippets):
---
{context}
---
Task: Generate test cases for the scope: "{scope}".
If acceptance criteria (AC) appear in the context, map each test to at least one AC id/phrase.

Output (Markdown):
- Feature: <name>
- Scope: <scope>
- Assumptions/Notes: <gaps or clarifications needed>
- Test Cases (table):
  | ID | Title | Type (pos/neg/edge) | Pre-Conditions | Steps | Expected Result | Priority (H/M/L) | MapsTo (AC id/phrase) |
After the table:
- Coverage: bullets of ACs covered and any missing ACs
- Additional Negative/Edge Ideas: bullets
"""

def make_context(hits, max_chars=1000):
    blocks=[]
    for h in hits:
        t = h["text"].strip()
        if len(t) > max_chars: t = t[:max_chars] + "..."
        blocks.append(f"[{h['source']} • score={h['score']:.2f}]\n{t}")
    return "\n\n".join(blocks)

def generate_test_cases(scope_query, k=8, use="local"):
    hits = retrieve(scope_query, k=k)
    ctx = make_context(hits)
    messages = [
        {"role":"system","content":TESTCASE_SYSTEM},
        {"role":"user","content":TESTCASE_USER_TEMPLATE.format(context=ctx, scope=scope_query)}
    ]
    out = chat_local(messages)  # or chat_api(messages)
    return out, hits

"""## Defining function to save model outputs to drive."""

import os, time, re
import pandas as pd

def parse_markdown_table(md_text: str):
    """
    Extracts the first Markdown table from LLM output and returns as DataFrame.
    Assumes '|' separated table rows with a header line.
    """
    lines = [ln.strip() for ln in md_text.splitlines() if ln.strip().startswith("|")]
    if not lines:
        return pd.DataFrame()

    # Remove markdown alignment row (| --- | --- | etc.)
    table_lines = [ln for ln in lines if not re.match(r'^\|\s*:?-+:?\s*\|', ln)]
    rows = [[c.strip() for c in ln.strip("|").split("|")] for ln in table_lines]

    if len(rows) < 2:
        return pd.DataFrame()

    header, body = rows[0], rows[1:]
    return pd.DataFrame(body, columns=header)

def generate_and_save(scope_query, k=8, use="local"):
    # Run your existing generator
    md_text, evidence = generate_test_cases(scope_query, k=k, use=use)

    # Timestamped filenames
    ts = time.strftime("%Y%m%d-%H%M%S")
    base = scope_query.strip().replace(" ", "_")[:50]

    md_path = os.path.join(OUT_DIR, f"{base}_{ts}.md")
    ev_path = os.path.join(OUT_DIR, f"{base}_{ts}_evidence.txt")
    csv_path = os.path.join(OUT_DIR, f"{base}_{ts}.csv")

    # Save markdown
    with open(md_path, "w", encoding="utf-8") as f:
        f.write(md_text)

    # Save evidence
    with open(ev_path, "w", encoding="utf-8") as f:
        for h in evidence:
            f.write(f"[{h['source']} • score={h['score']:.2f}]\n{h['text']}\n\n")

    # Parse table → CSV
    df = parse_markdown_table(md_text)
    if not df.empty:
        df.to_csv(csv_path, index=False)
        print(f"✅ Saved CSV with {len(df)} rows: {csv_path}")
    else:
        print("⚠️ No markdown table parsed, skipping CSV export.")

    print(f"✅ Saved outputs:\n- {md_path}\n- {ev_path}")
    return md_text, evidence

#trying it
md_text, evidence = generate_test_cases(
    "User registration: email verification, strong password policy, duplicate email handling",
    k=10, use="local"
)
print(md_text[:1500])

print(evidence)

#trying it
md_text, evidence = generate_test_cases(
    "User login: email verification, wrong credentials, password change",
    k=10, use="local"
)
print(md_text[:1500])

print(evidence)